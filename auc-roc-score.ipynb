{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# AUC-ROC Score \n### The Receiver Operator Characteristic (ROC) curve is an evaluation metric for binary classification problems. It is a probability curve that plots the TPR against FPR at various threshold values and essentially separates the ‘signal’ from the ‘noise’. The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve.\n\n### The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes.\n\n### When AUC = 1, then the classifier is able to perfectly distinguish between all the Positive and the Negative class points correctly. If, however, the AUC had been 0, then the classifier would be predicting all Negatives as Positives, and all Positives as Negatives.\n\n### When 0.5<AUC<1, there is a high chance that the classifier will be able to distinguish the positive class values from the negative class values. This is so because the classifier is able to detect more numbers of True positives and True negatives than False negatives and False positives.\n\n### When AUC=0.5, then the classifier is not able to distinguish between Positive and Negative class points. Meaning either the classifier is predicting random class or constant class for all the data points.\n\n### So, the higher the AUC value for a classifier, the better its ability to distinguish between positive and negative classes.\n\n### Let's use AUC ROC score to check the performance of our models.\n\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/AUC3.png)\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/AUC3.png)\n![](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/06/AUC2.png)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-30T02:31:48.737682Z","iopub.execute_input":"2021-08-30T02:31:48.738474Z","iopub.status.idle":"2021-08-30T02:31:50.933080Z","shell.execute_reply.started":"2021-08-30T02:31:48.738337Z","shell.execute_reply":"2021-08-30T02:31:50.931695Z"}}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n## Train Test Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=27)\nmodel1 = LogisticRegression(solver='liblinear')\nmodel2 = KNeighborsClassifier(n_neighbors=4)\nmodel3 = DecisionTreeClassifier(max_depth =3, random_state = 42)\nmodel4 = RandomForestClassifier(max_depth=2, random_state=0)\nmodel5 = GaussianNB()\n\n\nmodel1.fit(X_train, y_train)\nmodel2.fit(X_train, y_train)\nmodel3.fit(X_train, y_train)\nmodel4.fit(X_train, y_train)\nmodel5.fit(X_train, y_train)\n\n\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_prob1 = model1.predict_proba(X_test)\npred_prob2 = model2.predict_proba(X_test)\npred_prob3 = model3.predict_proba(X_test)\npred_prob4 = model4.predict_proba(X_test)\npred_prob5 = model5.predict_proba(X_test)\n\n\nfrom sklearn.metrics import roc_curve\n #roc curve for models\nfpr1, tpr1, thresh1 = roc_curve(y_test, pred_prob1[:,1], pos_label=1)\nfpr2, tpr2, thresh2 = roc_curve(y_test, pred_prob2[:,1], pos_label=1)\nfpr3, tpr3, thresh3 = roc_curve(y_test, pred_prob3[:,1], pos_label=1)\nfpr4, tpr4, thresh4 = roc_curve(y_test, pred_prob4[:,1], pos_label=1)\nfpr5, tpr5, thresh5 = roc_curve(y_test, pred_prob5[:,1], pos_label=1)\n\n\n\n# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)\n\nfrom sklearn.metrics import roc_auc_score\n\n# auc scores\nauc_score1 = roc_auc_score(y_test, pred_prob1[:,1])\nauc_score2 = roc_auc_score(y_test, pred_prob2[:,1])\nauc_score3 = roc_auc_score(y_test, pred_prob3[:,1])\nauc_score4 = roc_auc_score(y_test, pred_prob4[:,1])\nauc_score5 = roc_auc_score(y_test, pred_prob5[:,1])\n\n\n\nprint('the AUC ROC score for Logistic Regression Model is -      ',auc_score1)\nprint(\"\")\nprint('the AUC ROC score for K Neighbors Classifier Model is -   ',auc_score2)\nprint(\"\")\nprint('the AUC ROC score for  Decision Tree Model is -  ',auc_score3)\nprint(\"\")\nprint('the AUC ROC score for Random Forest Classifier Model is -   ',auc_score4)\nprint(\"\")\nprint('the AUC ROC score for Naive Bayes Classsifier Model is -   ',auc_score5)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# matplotlib\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn')\n\n# plot roc curves\nplt.plot(fpr1, tpr1, linestyle='--',color='orange', label='Logistic Regression')\nplt.plot(fpr2, tpr2, linestyle='--',color='red', label='KNN')\nplt.plot(fpr3, tpr3, linestyle='--',color='yellow', label='DT')\nplt.plot(fpr4, tpr4, linestyle='--',color='green', label='RF')\nplt.plot(fpr5, tpr5, linestyle='--',color='pink', label='NaiveBayes')\n\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC curve')\n# x label\nplt.xlabel('False Positive Rate')\n# y label\nplt.ylabel('True Positive rate')\n\nplt.legend(loc='best')\nplt.savefig('ROC',dpi=300)\nplt.show();\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}